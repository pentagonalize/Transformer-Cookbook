%-----------------------------------
%
\chapter{Self-Attention Layers}
%
%-----------------------------------

\section{Definitions}

\emph{Scaled dot-product self-attention} with $d$ input/output dimensions and $\dhid$ key/value dimensions is a length-preserving function
\begin{align}
  \att \colon (\R^d)^* &\to (\R^d)^* \notag \\
  \mathbf{X} &\mapsto \mathbf{Y} \\
  s\elt{i,j} &= \frac{\qproj(\mathbf{X}\elt{i}) \cdot \kproj(\mathbf{X}\elt{j})}{\sqrt\dhid} \label{eq:att_logit} \\
  \alpha\elt{i,:} &= \softmax(s\elt{i,:}) \label{eq:att_weight} \\
  &= \frac{\exp s\elt{i,:}}{\displaystyle\sum_{j \in [d]} \exp s\elt{i,j}} \\
  \mathbf{Y}\elt{i} &= \sum_{j\in[n]} \alpha\elt{i,j} \, \vproj (\mathbf{X}\elt{j}) \\
  \intertext{with parameters}
  \qproj,\kproj &\in \R^{\dhid \times d} \notag \\
  \vproj &\in \R^{d \times d} \notag
\end{align}
We call the $s\elt{i,j}$ the \emph{attention scores}, and we call the $\alpha\elt{i,j}$ the \emph{attention weights}.

Real transformers use \emph{multi-head} self-attention, but we don't use it because it can be emulated using single-head self-attention.

\paragraph{Attention masking} In \emph{future-masked} (also known as \emph{causally}-masked) self attention, a term $m(i,j)$ is added to \cref{eq:att_logit} to force every position to attend only to preceding positions:
\begin{equation}
    m(i,j) =
    \begin{cases}
        0 & \text{if $j \le i$} \\
        -\infty & \text{otherwise.}
    \end{cases}
\end{equation}
(We define $\exp (-\infty) = 0$.)
Some papers use \emph{strict} future-masking, that is, $m(i,j) = 0$ iff $j<i$,
and occasionally \emph{past}-masking ($j \ge i$) and strict past-masking ($j>i$).

\iffalse
Our definition deviates from the original formulation \citep{vaswani-etal-2017 attention} and commonly-used implementations in a few ways:
\begin{compactenum}
    \item Originally, $\qproj$ and $\kproj$ had $d_\text{k}$ output dimensions and $\vproj$ had $d_\text{v}$ output dimensions, and $d_\text{k}=d_\text{v}$. Here, we set all three to $\dhid$, aligning with common practice in implementations.
    \item Originally, $\outmap$ was defined as part of multi-head attention, but we have moved it into attention.
    This does not change the model.
    \item \Cref{eq:att_weight} is equivalent to the more usual matrix form $\softmax\mleft(\frac{\mat{Q}\mat{K}^\top}{\sqrt{\dhid}}\mright)\mat{V}$, where $\mat{Q} \in \R^{n\times\dhid}$ is a matrix whose $i$-th row corresponds to $\qproj(\mathbf{X})$, and similarly for $\mat{K}$ and $\mat{V}$.
    \item Some implementations use affine instead of linear transformations for $\qproj$, $\kproj$, and $\vproj$, and some include a bias term in $s_{i,j}(\mathbf{X})$.
\end{compactenum}
Since our focus here is solely on scaled dot-product attention, we will refer to it simply as \emph{attention}.
\fi

\iffalse
\insentpara{Multi-head attention} with $\dhid$ key/value dimensions per head is the sum of $\numheads$ attentions with $\dhid$ key/value dimensions: %, each with $\din$ input dimensions, $\dout$ output dimensions, and $\dhid$ key/value dimensions:
\begin{equation*}
\mha(\mathbf{x},\mathbf{X})= \sum_{h\in[\numheads]}\att_h(\mathbf{x},\mathbf{X}).
\end{equation*} 
Multi-head self attention is defined analogously.
This is equivalent to the original formulation, which concatenated the outputs of the heads and passed the result through a shared, larger, $\outmap$.
\fi

\paragraph{Hard attention}
Some theoretical analyses simplify attention by replacing the softmax with variants that focus attention only on the position(s) with the maximum value, breaking ties in various ways.

For any vector $\mathbf{x} \in \R^d$, define $M(\mathbf{x}) = \{i \in [n] \mid \forall j \in [n], \mathbf{x}\elt{j} \le \mathbf{x}\elt{i}\}$ to be the set of indices of the maximal elements of $\mathbf{x}$.
In \emph{leftmost}-hard attention, the leftmost maximal element is used, replacing \cref{eq:att_weight} with:
\begin{align}
    \alpha\elt{i,j} &= \mathbb{I}[j = \min M(s[i,:])] \\
\intertext{whereas in \emph{average}-hard attention, the maximal elements share weight equally:}
    \alpha\elt{i,j} &= \frac{\mathbb{I}[j \in M(s[i,:])]}{|M(s[i,:])|}.
\end{align}

Leftmost-hard attention was previously called 
\emph{hard} attention by \citet{hahn-2020-theoretical} and
\emph{unique-hard} attention by \citet{hao-etal-2022-circuits}.
One may also consider rightmost-hard attention, in which the rightmost maximal element is used.
Average-hard attention was also called 
\emph{hard} attention by \citet{perez-etal-2021-turing} and
\emph{saturated} attention by \citet{merrill-etal-2021-saturated-transformers}, and has been argued to be a realistic approximation to how trained transformers behave in practice \citep{merrill-etal-2021-effects}.
Neither type of hard attention should be confused with the concept of hard attention used in computer vision \citep[e.g.,][]{xu+:2015}.

\section{Trivial Cases}

Identity function: zero values, use residual connection

Uniform attention: zero queries and/or keys

\section{Counting}
\label{sec:attn_counting}

\section{Final Position}
\label{sec:attn_final_position}

As described in \citet{barcelo-etal-2024-logical}, one might like the integer value $n$ to be accessible at every position. To do this, all we need is the position embedding to contain $i$ and $1/(i+1)$, and then we can set the queries and keys as follows:

\begin{align*}
  q_i &= \begin{bmatrix} -\frac{1}{i+1} \end{bmatrix} \\
  k_j &= \begin{bmatrix} \frac{1}{j+1} \end{bmatrix}
\end{align*}

For every $i$, the dot product $s_{i,j} = q_i \cdot k_j = -\frac{1}{(i+1)(j+1)}$ is maximized when $j=n$, so this allows a hard-attention layer to attend uniquely to the final position and retrive the value fo $n$ from the postion embedding

\section{Table Lookup}

Given that position $i$ stores some integer $a_i$, a table lookup is a self-attention layer in which each position $i$ attends to position $a_i$.
If $f \colon [n] \to \R$ is any function and we can use the position embedding to store $f(j)$ at position $j$, then table lookup makes it possible to compute $f(a_i)$ at each position $i$.

\iffalse
\subsection{Single query}

If every position has the same query ($q^{(i)} = q$ for all $i$) and there is a minimum distance $\delta$ between all keys ($|k^{(i)}-k^{(j)}| \ge \delta$ for all $i,j$), then we can just use a FFNN to compute $\mathbb{I}[k^{(i)} = q^{(i)}] \delta$, which is piecewise linear \citep{chiang-cholak-2022-parity}:
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[xtick={-1,0,1},xticklabels={$-\delta$,$0$,$\delta$,ytick={0,1},yticklabels={$0$,$\delta$}}}]
      \addplot[mark=none] coordinates { (-5,0) (-1,0) (0,1) (1,0) (5,0) };
    \end{axis}
  \end{tikzpicture}
\end{center}
(Since this doesn't use attention, it doesn't strictly belong in this chapter.)
\fi

\subsection{Layernorm hash}

\Citet{merrill-sabharwal-2024-cot} store integer $a_i$ as
\begin{align*}
  \textnormal{lh}(a_i) &= \textnormal{LayerNorm}\left(\begin{bmatrix} a_i \\ 1 \\ -a_i \\ -1 \end{bmatrix}\right) \\
  &= \frac1{\sqrt{2a_i^2+2}} \begin{bmatrix} a_i \\ 1 \\ -a_i \\ -1 \end{bmatrix}.
\end{align*}
One advantage of this representation is that if we're only able to compute $a_i/i$ and $1/i$ (as is common when counting positions using uniform attention), we can still compute $\textnormal{lh}(a_i)$. Another advantage is that $\textnormal{lh}(a)\cdot\textnormal{lh}(b)$ is uniquely maximized when $a=b$. That is, for any $i>0$, we have that $\textnormal{lh}(a_i)=\textnormal{LayerNorm}(a_i/i,1/i,-a_i/i,-1/i) = \textnormal{LayerNorm}(a_i,1,-a_i,-1)$. So if we let
\begin{align*}
  q_i &= \textnormal{lh}(a_i) \\
  k_j &= \textnormal{lh}(j)
\end{align*}
then the dot product $s_{i,j} = \textnormal{lh}(a_i) \cdot \textnormal{lh}(j)$ is uniquely maximized when $a_i = j$, so that self-attention will retrieve the value $v_j$ such that $a_i = j$. In the setting of hard-attention, this allows us to select only the position vector at position $j=a_i$, after which the the value of $f(j)$ can be retrieved by appropriately setting $W^{(V)}$.

The disadvantage of this representation is that it requires the ability to selectively apply $\textnormal{LayerNorm}$ to just four components of a vector. We can't do this with standard transformer layers (either pre-norm or post-norm), but we can if we use pre-norm \emph{and} insert a linear transformation $\mathbf{W}^{(\textnormal{N})}$ before the layer normalization:
\begin{align*}
  \mathbf{y} &= \textnormal{Sublayer}(\textnormal{LayerNorm}(\mathbf{W}^{(\textnormal{N})} \mathbf{x})) + \mathbf{x}.
\end{align*}

\subsection{Quadratic maximization}

\Citet{barcelo-etal-2024-logical} include $j$ and $j^2$ in the position embedding, allowing table lookup as follows:
\begin{align*}
  q_i &= \begin{bmatrix} a_i \\ 1 \end{bmatrix} \\
  k_j &= \begin{bmatrix} 2j \\ -j^2 \end{bmatrix}
\end{align*}
The dot product $s_{i,j}$ is $2a_ij - j^2$, which is uniquely maximized when $j=a_i$.
This is true even if either $q_i$ or $k_j$ is scaled by some factor.

\iffalse
\subsection{$-|\text{Dot-product}|$ attention}

\citep{perez-etal-2021-turing}
\fi

\section{Dot-Product and Disjunctive Normal Form}
\label{sec:att_dnf}

\section{Tie Breaking}

When using \emph{average-hard} attention, sometimes we want to be able to simulate \emph{leftmost-hard} or \emph{rightmost-hard} attention.
