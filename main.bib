@inproceedings{vaswani-etal-2017-attention,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS)},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{arora+:2018,
  author    = {Raman Arora and Amitabh Basu and Poorya Mianjy and Anirbit Mukherjee},
  title     = {Understanding Deep Neural Networks with Rectified Linear Units},
  year      = 2018,
  booktitle = {Proceedings of the Sixth International Conference on Learning Representations (ICLR)},
  url       = {https://openreview.net/forum?id=B1J_rgWRW}
}

@inproceedings{feng2024towards,
  title     = {Towards Revealing the Mystery behind {C}hain of {T}hought: A Theoretical Perspective},
  author    = {Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
  year      = {2023},
  url       = {https://papers.nips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html}
}

@inproceedings{akyurek2022learning,
  title     = {What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author    = {Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle = {The Eleventh International Conference on Learning Representations (ICLR)},
  year      = {2023},
  url       = {https://openreview.net/forum?id=0g0X4H8yN4I}
}

@misc{hendrycks2023,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Dan Hendrycks and Kevin Gimpel},
  year          = {2023},
  eprint        = {1606.08415},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  doi           = {10.48550/arXiv.1606.08415}
}

@article{Fukushima1975,
  author   = {Fukushima, Kunihiko},
  title    = {Cognitron: A self-organizing multilayered neural network},
  journal  = {Biological Cybernetics},
  year     = {1975},
  month    = {Sep},
  day      = {01},
  volume   = {20},
  number   = {3},
  pages    = {121-136},
  abstract = {A new hypothesis for the organization of synapses between
              neurons is proposed: ``The synapse from neuron x to neuron y is
              reinforced when x fires provided that no neuron in the vicinity
              of y is firing stronger than y''. By introducing this hypothesis,
              a new algorithm with which a multilayered neural network is
              effectively organized can be deduced. A self-organizing
              multilayered neural network, which is named ``cognitron'', is
              constructed following this algorithm, and is simulated on a
              digital computer. Unlike the organization of a usual brain models
              such as a three-layered perceptron, the self-organization of a
              cognitron progresses favorably without having a ``teacher'' which
              instructs in all particulars how the individual cells respond.
              After repetitive presentations of several stimulus patterns, the
              cognitron is self-organized in such a way that the receptive
              fields of the cells become relatively larger in a deeper layer.
              Each cell in the final layer integrates the information from
              whole parts of the first layer and selectively responds to a
              specific stimulus pattern or a feature.},
  issn     = {1432-0770},
  doi      = {10.1007/BF00342633},
  url      = {https://doi.org/10.1007/BF00342633}
}

@inproceedings{chiang-cholak-2022-parity,
  title      = {Overcoming a Theoretical Limitation of Self-Attention},
  author     = {Chiang, David  and
                Cholak, Peter},
  booktitle  = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  month      = may,
  year       = {2022},
  xaddress   = {Dublin, Ireland},
  xpublisher = {Association for Computational Linguistics},
  url        = {https://aclanthology.org/2022.acl-long.527},
  doi        = {10.18653/v1/2022.acl-long.527},
  pages      = {7654--7664}
}

@inproceedings{merrill-sabharwal-2024-cot,
  author    = {William Merrill and Ashish Sabharwal},
  year      = 2024,
  title     = {The Expressive Power of Transformers with Chain of Thought},
  booktitle = {Proceedings of the Twelfth International Conference on Learning Representations (ICLR)},
  arxiv_url = {https://arxiv.org/abs/2310.07923},
  url       = {https://openreview.net/forum?id=NjNGlPh8Wh},
  pdf       = {https://openreview.net/pdf?id=NjNGlPh8Wh}
}

@inproceedings{barcelo-etal-2024-logical,
  title     = {Logical Languages Accepted by Transformer Encoders with Hard Attention},
  author    = {Pablo Barcel{\'o} and Alexander Kozachinskiy and Anthony Widjaja Lin and Vladimir Podolskii},
  year      = 2024,
  booktitle = {Proceedings of the Twelfth International Conference on Learning Representations (ICLR)},
  arxiv_url = {https://arxiv.org/abs/2310.03817},
  url       = {https://openreview.net/forum?id=gbrHZq07mq},
  pdf       = {https://openreview.net/pdf?id=gbrHZq07mq}
}

% Note that the version below has an error that results in omitting the proof that Majority can be recognized by a Transformer
@article{perez-etal-2021-turing,
  author    = {Jorge P{\'{e}}rez and
               Pablo Barcel{\'{o}} and
               Javier Marinkovic},
  title     = {Attention is {T}uring-Complete},
  journal   = {Journal of Machine Learning Research},
  volume    = {22},
  pages     = {75:1--75:35},
  year      = {2021},
  url       = {http://jmlr.org/papers/v22/20-302.html},
  timestamp = {Mon, 31 Jan 2022 17:23:36 +0100},
  biburl    = {https://dblp.org/rec/journals/jmlr/PerezBM21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chiang+:icml2023,
  xurl      = {https://arxiv.org/abs/2301.10743},
  url       = {https://proceedings.mlr.press/v202/chiang23a.html},
  author    = {Chiang, David and Cholak, Peter and Pillay, Anand},
  title     = {Tighter Bounds on the Expressivity of Transformer Encoders},
  year      = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages     = {5544--5562}
}

@article{yang2024counting,
  title   = {Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers},
  author  = {Yang, Andy and Chiang, David},
  journal = {arXiv preprint arXiv:2404.04393},
  year    = {2024}
}

@inproceedings{geva-etal-2021-transformer,
  title     = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author    = {Geva, Mor  and
               Schuster, Roei  and
               Berant, Jonathan  and
               Levy, Omer},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.446},
  doi       = {10.18653/v1/2021.emnlp-main.446},
  pages     = {5484--5495},
  abstract  = {Feed-forward layers constitute two-thirds of a transformer model{'}s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys{'} input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model{'}s layers via residual connections to produce the final output distribution.}
}

@article{hornik1989multilayer,
  title     = {Multilayer feedforward networks are universal approximators},
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal   = {Neural networks},
  volume    = {2},
  number    = {5},
  pages     = {359--366},
  year      = {1989},
  publisher = {Elsevier}
}

@inproceedings{shiv2019,
  author = {Shiv, Vighnesh and Quirk, Chris},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Novel positional encodings to enable tree-based transformers},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf},
  volume = {32},
  year = {2019}
}
